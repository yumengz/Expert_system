ç”¨docker appå®‰è£…k8s
Settings => Kubernetes => enable => reset => apply and restart

kubectl get nodes

==> é•¿çƒè¿™æ ·
(base) Wenqings-MacBook-Pro:open-webui wenqings$ kubectl get nodes
NAME             STATUS   ROLES           AGE   VERSION
docker-desktop   Ready    control-plane   29s   v1.30.5

==> æä¸å®šçš„è¯ï¼Œåˆ çƒè¿™é‡Œé¢çš„æ–‡ä»¶ï¼Œ å†åœ¨docker appé‡Œé‡è£…k8s
~/.kube/config

brew install kustomize  # macOS
kustomize version
æˆ‘ç”µè„‘é•¿çƒè¿™æ ·
==> v5.6.0

### Installing Both Ollama and Open WebUI Using Kustomize

For cpu-only pod

```bash
å®˜æ–¹æŒ‡å—ï¼š
kubectl apply -f ./kubernetes/manifest/base
è¡Œä¸é€šï¼Œå»æ‰validation
kubectl apply -f ./kubernetes/manifest/base --validate=false
ä»¥ä¸Šä¸¤ä¸ªéƒ½è¡Œä¸é€šï¼Œè¯•è¯•è¿™ä¸ª
kubectl apply -k ./kubernetes/manifest/base --validate=false

Why?
-f applies raw YAML files, while -k is required for kustomization.yaml.

Check the Podsï¼Œ Make sure all pods are in the Running or Ready state. ï¼š
kubectl get pods -n open-webui
==>
(base) Wenqings-MacBook-Pro:open-webui wenqings$ kubectl get pods -n open-webui
NAME                                     READY   STATUS    RESTARTS   AGE
ollama-0                                 1/1     Running   0          17m
open-webui-deployment-86bf9df567-2wqzs   1/1     Running   0          18m


Check the Services
kubectl get svc -n open-webui

(base) Wenqings-MacBook-Pro:open-webui wenqings$ kubectl get svc -n open-webui
NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
ollama-service       ClusterIP   10.110.13.17    <none>        11434/TCP        19m
open-webui-service   NodePort    10.106.144.57   <none>        8080:31817/TCP   20m

Look for the open-webui-service and ollama-service. If they are using ClusterIP, you may need to expose them.

To expose Open WebUI locally using a port forward:
kubectl port-forward -n open-webui service/open-webui-service 6666:8080

(base) Wenqings-MacBook-Pro:open-webui wenqings$ kubectl port-forward -n open-webui service/open-webui-service 6666:8080
Forwarding from 127.0.0.1:6666 -> 8080
Forwarding from [::1]:6666 -> 8080

Now, you should be able to access Open WebUI at: http://localhost:6666 ==> Not work

Everything looks good in your Kubernetes setupâ€”both your pods are running, and your service is exposing port 8080 via NodePort (31817). However, since the service type is NodePort, your port-forwarding might not be necessary.
è¿™ç©æ„æˆåŠŸäº†ï¼š
http://localhost:31817/

è¿˜æœ‰ä»€ä¹ˆä¸æ‡‚çš„ï¼Œé—®chatgptå§ã€‚ã€‚ã€‚ 
è¯¥é¡¹ç›®åŸºäºyaml fileéƒ¨ç½²äºk8sï¼Œ å¯ä»¥é€šè¿‡ä»¥ä¸Šæ–¹æ³•æœ¬åœ°éƒ¨ç½²ï¼Œå°†æ¥ç›´æ¥åˆ©ç”¨tektonå•¥çš„éƒ¨ç½²äºäº‘ç«¯k8s


```

For gpu-enabled pod

```bash
kubectl apply -k ./kubernetes/manifest
```

### Installing Both Ollama and Open WebUI Using Helm

Package Helm file first

```bash
helm package ./kubernetes/helm/
```

For cpu-only pod

```bash
helm install ollama-webui ./ollama-webui-*.tgz
```

For gpu-enabled pod

```bash
helm install ollama-webui ./ollama-webui-*.tgz --set ollama.resources.limits.nvidia.com/gpu="1"
```

Check the `kubernetes/helm/values.yaml` file to know which parameters are available for customization





ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥
npm install -g pnpm
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
pnpm install

ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨if pnpm install error, try theseğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨
curl -fsSL https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.4/install.sh | bash
source ~/.bashrc 
nvm install 20
nvm use 20
node -v
pnpm install



1ï¸âƒ£ Running in Development Mode (Local)

Front end


cp -RPp .env.example .env
npm install
npm run dev

rm -rf node_modules package-lock.json
npm cache clean --force




1ï¸âƒ£ Start the Backend
Go to the backend directory:


cd backend
conda create --name open-webui python=3.11
conda activate open-webui

pip install -r requirements.txt -U

sh dev.sh

Kill the port
lsof -i :8080

lsof -i :5175


kill -9 22048




rm -rf node_modules package-lock.json .vite
npm cache clean --force
npm install

Clear backend cache:

cd backend
rm -rf __pycache__
Restart everything:

cd backend
sh dev.sh
Then, restart the frontend:

npm run dev

